{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/wesgurnee/Documents/mechint/sparse_probing/sparse-probing\n"
     ]
    }
   ],
   "source": [
    "cd .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# change to where ever you have your feature datasets saved\n",
    "import os\n",
    "#os.environ['HF_DATASETS_CACHE'] = '/Users/katherineharvey/sparse-probing-4/data/feature_datasets'''\n",
    "os.environ['HF_DATASETS_CACHE'] = '/Users/wesgurnee/Documents/mechint/sparse_probing/sparse-probing/downloads'\n",
    "\n",
    "os.environ['TRANSFORMERS_OFFLINE'] = '1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "from load import *\n",
    "from probing_datasets import language_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "NATURAL_LANGS_UNABBREVIATED = {\n",
    "    'bg': 'Bulgarian',\n",
    "    'de': 'German',\n",
    "    'es': 'Spanish',\n",
    "    'fr': 'French',\n",
    "    'lt': 'Lithuanian',\n",
    "    'pl': 'Polish',\n",
    "    'sk': 'Slovak',\n",
    "    'da': 'Danish',\n",
    "    'en': 'English',\n",
    "    'fi': 'Finnish',\n",
    "    'it': 'Italian',\n",
    "    'nl': 'Dutch',\n",
    "    'ro': 'Romanian',\n",
    "    'sv': 'Swedish',\n",
    "    'cs': 'Czech',\n",
    "    'el': 'Greek',\n",
    "    'et': 'Estonian',\n",
    "    'hu': 'Hungarian',\n",
    "    'lv': 'Lativian',\n",
    "    'pt': 'Portuguese',\n",
    "    'sl': 'Slovenian'\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Overview\n",
    "We need nice tables for the paper which describe all of the models and datasets we used/made. You might read up a _little_ bit about how to make tables manually (see [here](https://www.overleaf.com/learn/latex/Tables)) but you will want to do as much as possible programatically (such that if we want to make any changes we can just rerun the cell). My recommendation to start is to put everything into a pandas dataframe and then run `df.to_latex()`. You can then copy it into the overleaf to see how it renders (though you can also render it in a markdown cell in jupyter). I expect chatGPT to be fairly helpful with this so be mindful how you might factor the problem into promptable chunks.\n",
    "\n",
    "## Model Table\n",
    "Here we want a table similar to the models table [here](https://raw.githubusercontent.com/EleutherAI/pythia/main/README.md). We use the 70m-6.9b models (which are not deduped). Only include the architecture params (ie. anything that starts with n_ or d_), as well as the number of total neurons (4 * d_model * n_layers)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Params</th>\n",
       "      <th>n_layers</th>\n",
       "      <th>d_model</th>\n",
       "      <th>n_heads</th>\n",
       "      <th>d_head</th>\n",
       "      <th>Total Number of Neurons</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Pythia 70M</td>\n",
       "      <td>6</td>\n",
       "      <td>512</td>\n",
       "      <td>8</td>\n",
       "      <td>64</td>\n",
       "      <td>12288</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Pythia 160M</td>\n",
       "      <td>12</td>\n",
       "      <td>768</td>\n",
       "      <td>12</td>\n",
       "      <td>64</td>\n",
       "      <td>36864</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Pythia 410M</td>\n",
       "      <td>24</td>\n",
       "      <td>1024</td>\n",
       "      <td>16</td>\n",
       "      <td>64</td>\n",
       "      <td>98304</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Pythia 1B</td>\n",
       "      <td>16</td>\n",
       "      <td>2048</td>\n",
       "      <td>8</td>\n",
       "      <td>256</td>\n",
       "      <td>131072</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Pythia 1.4B</td>\n",
       "      <td>24</td>\n",
       "      <td>2048</td>\n",
       "      <td>16</td>\n",
       "      <td>128</td>\n",
       "      <td>196608</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Pythia 2.8B</td>\n",
       "      <td>32</td>\n",
       "      <td>2560</td>\n",
       "      <td>32</td>\n",
       "      <td>80</td>\n",
       "      <td>327680</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Pythia 6.9B</td>\n",
       "      <td>32</td>\n",
       "      <td>4096</td>\n",
       "      <td>32</td>\n",
       "      <td>128</td>\n",
       "      <td>524288</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        Params  n_layers  d_model  n_heads  d_head  Total Number of Neurons\n",
       "0   Pythia 70M         6      512        8      64                    12288\n",
       "1  Pythia 160M        12      768       12      64                    36864\n",
       "2  Pythia 410M        24     1024       16      64                    98304\n",
       "3    Pythia 1B        16     2048        8     256                   131072\n",
       "4  Pythia 1.4B        24     2048       16     128                   196608\n",
       "5  Pythia 2.8B        32     2560       32      80                   327680\n",
       "6  Pythia 6.9B        32     4096       32     128                   524288"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "model_df = pd.DataFrame({\n",
    "    'Params': ['Pythia 70M','Pythia 160M', 'Pythia 410M', 'Pythia 1B','Pythia 1.4B', 'Pythia 2.8B', 'Pythia 6.9B'], \n",
    "    'n_layers': [6, 12, 24, 16, 24, 32, 32],\n",
    "    'd_model': [512, 768, 1024, 2048, 2048, 2560, 4096],\n",
    "    'n_heads': [8, 12, 16, 8, 16, 32, 32],\n",
    "    'd_head': [64, 64, 64, 256, 128, 80, 128],\n",
    "    'Total Number of Neurons': [12288, 36864, 98304, 131072, 196608, 327680, 524288]\n",
    "})\n",
    "model_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\begin{tabular}{lrrrrr}\n",
      "\\toprule\n",
      "     Params &  n\\_layers &  d\\_model &  n\\_heads &  d\\_head &  Total Number of Neurons \\\\\n",
      "\\midrule\n",
      " Pythia 70M &         6 &      512 &        8 &      64 &                    12288 \\\\\n",
      "Pythia 160M &        12 &      768 &       12 &      64 &                    36864 \\\\\n",
      "Pythia 410M &        24 &     1024 &       16 &      64 &                    98304 \\\\\n",
      "  Pythia 1B &        16 &     2048 &        8 &     256 &                   131072 \\\\\n",
      "Pythia 1.4B &        24 &     2048 &       16 &     128 &                   196608 \\\\\n",
      "Pythia 2.8B &        32 &     2560 &       32 &      80 &                   327680 \\\\\n",
      "Pythia 6.9B &        32 &     4096 &       32 &     128 &                   524288 \\\\\n",
      "\\bottomrule\n",
      "\\end{tabular}\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/zl/vrkthn715j97ns3mht1gpfm80000gn/T/ipykernel_3380/246342278.py:1: FutureWarning: In future versions `DataFrame.to_latex` is expected to utilise the base implementation of `Styler.to_latex` for formatting and rendering. The arguments signature may therefore change. It is recommended instead to use `DataFrame.style.to_latex` which also contains additional functionality.\n",
      "  model_latex_table = model_df.to_latex(index=False)\n"
     ]
    }
   ],
   "source": [
    "model_latex_table = model_df.to_latex(index=False)\n",
    "print(model_latex_table)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset Table\n",
    "Here we want a table which describes all of the datasets we made/use (listed below, though these could change). You can get started just creating a table for these, but for a few of them (potentially just EWT actually) we may want additional granularity. The columns we want are:\n",
    "* Dataset Name\n",
    "* Number of rows \n",
    "* Number of columns (i.e. number of tokens per row)\n",
    "* Number of total tokens (i.e. number of tokens which aren't 0 or 1)\n",
    "* List of the actual features (ie, the names of all the languages)\n",
    "* Original data source (e.g. \"pile-europarl\")\n",
    "You should get all of the \"number\" items programmatically, that is by actually loading every dataset and computing these things programmatically. For the list of features, depending on the dataset you should be able to get it programatically by looking at the column names in the dataset which are of the form `<feature_name>|probe_indices` or get the unique values from a relevant column in the dataset (like `lang` below) but if there are tricky edge cases you can hardcode some of them. For the original data source, you can just hardcode it in a dictionary (and you can make up data for now and I will put in the correct values).\n",
    "\n",
    "Again the goal is to programmatically construct a pandas dataframe with all this information, then generate a table, then potentially do something which makes it look nicer.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_datasets = [\n",
    "    'programming_lang_id.pyth.512.-1',\n",
    "    'natural_lang_id.pyth.512.-1',\n",
    "    'text_features.pyth.256.10000',\n",
    "    'counterfact.pyth.64.-1',\n",
    "    'distribution_id.pyth.512.-1',\n",
    "    'ewt.pyth.512.-1',\n",
    "    'compound_words.pyth.24.-1',\n",
    "    'latex.pyth.1024.-1'\n",
    "]\n",
    "\n",
    "dataset_sources = {\n",
    "    'programming_lang_id.pyth.512.-1': 'pile-github',\n",
    "    'natural_lang_id.pyth.512.-1': 'pile-europarl',\n",
    "    'text_features.pyth.256.10000': 'pile-test-all',\n",
    "    'counterfact.pyth.64.-1': 'pile-test-all',\n",
    "    'distribution_id.pyth.512.-1': 'pile-test-all',\n",
    "    'ewt.pyth.512.-1': 'EWT',\n",
    "    'compound_words.pyth.24.-1': 'pile-test-all',\n",
    "    'latex.pyth.1024.-1': 'pile-arxiv'\n",
    "}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# small helper function to unabbreviate languages\n",
    "def unabbreviate_languages(abbr_list):\n",
    "    unabbr_list = [NATURAL_LANGS_UNABBREVIATED.get(abbr, abbr) for abbr in abbr_list]\n",
    "    return \", \".join(unabbr_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_features(dataset):\n",
    "    fds = load_feature_dataset(dataset)\n",
    "    features = set()\n",
    "    for feature in fds.features:\n",
    "        if '|' in feature:\n",
    "            features.add(feature.split('|')[0])\n",
    "        elif feature in ('lang', 'relation_id', 'distribution', 'feature_name'):\n",
    "            features.update(set(fds[feature]))\n",
    "    \n",
    "    features = [item for item in features if item not in ['all_tokens', 'tokens', 'meta', 'text']]\n",
    "    return unabbreviate_languages(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/katherineharvey/sparse-probing-4/sparprob/lib/python3.9/site-packages/datasets/arrow_dataset.py:1533: FutureWarning: 'fs' was is deprecated in favor of 'storage_options' in version 2.8.0 and will be removed in 3.0.0.\n",
      "You can remove this warning by passing 'storage_options=fs.storage_options' instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Python, XML, Java, C++, HTML, C, Go, PHP, JavaScript'"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_features('programming_lang_id.pyth.512.-1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creates a list of all features that start with a certain prefix\n",
    "def filter_by_prefix(labels, prefix):\n",
    "    return [label for label in labels if label.startswith(prefix) and label.endswith('probe_classes')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_avg_positives():\n",
    "    average_pos = []\n",
    "    average_neg = []\n",
    "    ewt = load_feature_dataset('ewt.pyth.512.-1')\n",
    "    upos = filter_by_prefix(ewt.features, 'upos')\n",
    "    dep = filter_by_prefix(ewt.features, 'dep')\n",
    "    other = list(filter(lambda x: not x.startswith(\"upos\") and not x.startswith(\"dep\"), ewt.features))\n",
    "    for dataset in feature_datasets:\n",
    "        if dataset == 'programming_lang_id.pyth.512.-1' or dataset == 'natural_lang_id.pyth.512.-1' or dataset == 'distribution_id.pyth.512.-1':\n",
    "            average_pos.append(round(1/9, 2))\n",
    "            average_neg.append(round(8/9, 2))\n",
    "        elif dataset == 'text_features.pyth.256.10000' or dataset == 'latex.pyth.1024.-1':\n",
    "            fds = load_feature_dataset(dataset)\n",
    "            feature_names = [name for name in fds.features if '|probe_classes' in name]\n",
    "            total_positive = 0\n",
    "            total_negative = 0\n",
    "            total_count = 0\n",
    "            for feature in feature_names:\n",
    "                concat_tensor = torch.cat(fds[feature])\n",
    "                count_positive = (concat_tensor == 1).sum().item()\n",
    "                total_positive += count_positive\n",
    "                count_negative = (concat_tensor == -1).sum().item()\n",
    "                total_negative += count_negative\n",
    "                total_count += (concat_tensor == -1).sum().item() + count_positive\n",
    "                avg_pos = total_positive / total_count\n",
    "                avg_neg = total_negative / total_count\n",
    "            average_pos.append(round(avg_pos, 2))\n",
    "            average_neg.append(round(avg_neg, 2))\n",
    "        elif dataset == 'counterfact.pyth.64.-1':\n",
    "            average_pos.append(0)\n",
    "            average_neg.append(0)\n",
    "        elif dataset == 'ewt.pyth.512.-1':\n",
    "            fds = load_feature_dataset(dataset)\n",
    "            for features, feature_name in [(set(upos), 'upos'), (set(dep), 'dep'), (set(other), 'other')]:\n",
    "                feature_names = [name for name in features if '|probe_classes' in name]\n",
    "                total_positive = 0\n",
    "                total_negative = 0 \n",
    "                total_count = 0\n",
    "                for feature in feature_names:\n",
    "                    concat_tensor = torch.cat(fds[feature])\n",
    "                    count_positive = (concat_tensor == 1).sum().item()\n",
    "                    total_positive += count_positive\n",
    "                    count_negative = (concat_tensor == -1).sum().item()\n",
    "                    total_negative += count_negative\n",
    "                    total_count += (concat_tensor == -1).sum().item() + count_positive\n",
    "                avg_pos = total_positive / total_count\n",
    "                avg_neg = total_negative / total_count\n",
    "                average_pos.append(round(avg_pos, 1))\n",
    "                average_neg.append(round(avg_neg, 1))\n",
    "        elif dataset == 'compound_words.pyth.24.-1':\n",
    "            fds = load_feature_dataset('compound_words.pyth.24.-1')\n",
    "            avg_pos = 33559 / len(fds)\n",
    "            avg_neg = 1 - avg_pos\n",
    "            average_pos.append(round(avg_pos, 1))\n",
    "            average_neg.append(round(avg_neg, 1))\n",
    "    return average_pos, average_neg\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([0.11, 0.11, 0.26, 0, 0.11, 0.2, 0.2, 0.2, 0.2, 0.26],\n",
       " [0.89, 0.89, 0.74, 0, 0.89, 0.8, 0.8, 0.8, 0.8, 0.74])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compute_avg_positives()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dataset_info(feature_datasets, dataset_sources):\n",
    "    shapes = []\n",
    "    num_tokens_list = []\n",
    "    sources = []\n",
    "    col = []\n",
    "    features_dict = {}\n",
    "    avg_positives = []\n",
    "    \n",
    "    # info for all the datasets \n",
    "    for dataset in feature_datasets:\n",
    "        _fds = load_feature_dataset(dataset)\n",
    "        shapes.append(_fds.shape)\n",
    "        col.append(_fds['tokens'].shape[1])\n",
    "        num_tokens = (_fds['tokens'] > 1).numpy().sum()\n",
    "        num_tokens_list.append(num_tokens)\n",
    "        sources.append(dataset_sources[dataset])\n",
    "        features_dict[dataset] = get_features(dataset)\n",
    "        average_pos, average_neg = compute_avg_positives()\n",
    "\n",
    "    # creating a new list to contain all the data bc I did all the ewt stuff separately \n",
    "    dataset_data = []\n",
    "\n",
    "    # ewt \n",
    "    ewt = load_feature_dataset('ewt.pyth.512.-1')\n",
    "    upos = filter_by_prefix(ewt.features, 'upos')\n",
    "    dep = filter_by_prefix(ewt.features, 'dep')\n",
    "    other = list(filter(lambda x: not x.startswith(\"upos\") and not x.startswith(\"dep\") and x.endswith('probe_indices'), ewt.features))\n",
    "    # defined i so I can match with the correct value in average_list \n",
    "    i = 5\n",
    "    for features, feature_name in [(set(upos), 'upos'), (set(dep), 'dep'), (set(other), 'other')]:\n",
    "        i = i + 1\n",
    "        features_list = set()\n",
    "        for feature in features:\n",
    "            features_list.add(feature.split('|')[0])\n",
    "        dataset_data.append({\n",
    "            'Dataset': f'ewt.pyth.512.-1 ({feature_name})',\n",
    "            'Sequences': ewt.shape[0],\n",
    "            'Context Length': ewt['tokens'].shape[1],\n",
    "            'Non padding tokens': (ewt['tokens'] > 1).numpy().sum(),\n",
    "            'Total Features': len(set(features)),\n",
    "            'Average Class Balance': average_pos[i],\n",
    "            #'Average Number of Negative Examples' : average_neg[i],\n",
    "            'Features': unabbreviate_languages([item.split('|')[0] for item in features_list if item not in ['all_tokens', 'tokens', 'meta', 'text']]),\n",
    "            'Source': dataset_sources[feature_datasets[5]]\n",
    "        })\n",
    "        \n",
    "    # excluding values from ewt (probably a better way to do this) so the rest of the averages get mapped to the right dataset    \n",
    "    del average_pos[5:8]\n",
    "    del average_neg[5:8]\n",
    "    # defined j so I can match with the correct value in average_list\n",
    "    j = -1\n",
    "    # appends everything but ewt to the larger dataset\n",
    "    for i, dataset in enumerate(feature_datasets):\n",
    "        if i == 5: continue \n",
    "        j = j + 1\n",
    "        fds = load_feature_dataset(dataset)\n",
    "        dataset_data.append({\n",
    "            'Dataset': dataset,\n",
    "            'Sequences': fds.shape[0],\n",
    "            'Context Length': fds['tokens'].shape[1],\n",
    "            'Non padding tokens': (fds['tokens'] > 1).numpy().sum(),\n",
    "            'Total Features': len(get_features(dataset).split(', ')),\n",
    "            'Average Class Balance': average_pos[j],\n",
    "            #'Average Number of Negative Examples' : average_neg[j],\n",
    "            'Features': get_features(dataset),\n",
    "            'Source': dataset_sources[dataset]\n",
    "        })\n",
    "\n",
    "    dataset_df = pd.DataFrame(dataset_data, columns=['Dataset', 'Sequences', 'Context Length', 'Non padding tokens', 'Source', 'Average Class Balance', 'Total Features'])\n",
    "    return dataset_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/katherineharvey/sparse-probing-4/sparprob/lib/python3.9/site-packages/datasets/arrow_dataset.py:1533: FutureWarning: 'fs' was is deprecated in favor of 'storage_options' in version 2.8.0 and will be removed in 3.0.0.\n",
      "You can remove this warning by passing 'storage_options=fs.storage_options' instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "dataset_df = get_dataset_info(feature_datasets, dataset_sources)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Dataset</th>\n",
       "      <th>Sequences</th>\n",
       "      <th>Context Length</th>\n",
       "      <th>Non padding tokens</th>\n",
       "      <th>Source</th>\n",
       "      <th>Average Class Balance</th>\n",
       "      <th>Total Features</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ewt.pyth.512.-1 (upos)</td>\n",
       "      <td>1438</td>\n",
       "      <td>512</td>\n",
       "      <td>281044</td>\n",
       "      <td>EWT</td>\n",
       "      <td>0.20</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ewt.pyth.512.-1 (dep)</td>\n",
       "      <td>1438</td>\n",
       "      <td>512</td>\n",
       "      <td>281044</td>\n",
       "      <td>EWT</td>\n",
       "      <td>0.20</td>\n",
       "      <td>29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ewt.pyth.512.-1 (other)</td>\n",
       "      <td>1438</td>\n",
       "      <td>512</td>\n",
       "      <td>281044</td>\n",
       "      <td>EWT</td>\n",
       "      <td>0.20</td>\n",
       "      <td>22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>programming_lang_id.pyth.512.-1</td>\n",
       "      <td>5397</td>\n",
       "      <td>512</td>\n",
       "      <td>2757867</td>\n",
       "      <td>pile-github</td>\n",
       "      <td>0.11</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>natural_lang_id.pyth.512.-1</td>\n",
       "      <td>28084</td>\n",
       "      <td>512</td>\n",
       "      <td>14350924</td>\n",
       "      <td>pile-europarl</td>\n",
       "      <td>0.11</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>text_features.pyth.256.10000</td>\n",
       "      <td>10000</td>\n",
       "      <td>256</td>\n",
       "      <td>2248714</td>\n",
       "      <td>pile-test-all</td>\n",
       "      <td>0.26</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>counterfact.pyth.64.-1</td>\n",
       "      <td>43820</td>\n",
       "      <td>64</td>\n",
       "      <td>403331</td>\n",
       "      <td>pile-test-all</td>\n",
       "      <td>0.00</td>\n",
       "      <td>34</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>distribution_id.pyth.512.-1</td>\n",
       "      <td>8413</td>\n",
       "      <td>512</td>\n",
       "      <td>4299043</td>\n",
       "      <td>pile-test-all</td>\n",
       "      <td>0.11</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>compound_words.pyth.24.-1</td>\n",
       "      <td>167959</td>\n",
       "      <td>24</td>\n",
       "      <td>4031016</td>\n",
       "      <td>pile-test-all</td>\n",
       "      <td>0.20</td>\n",
       "      <td>21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>latex.pyth.1024.-1</td>\n",
       "      <td>4486</td>\n",
       "      <td>1024</td>\n",
       "      <td>4589178</td>\n",
       "      <td>pile-arxiv</td>\n",
       "      <td>0.26</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                           Dataset  Sequences  Context Length  \\\n",
       "0           ewt.pyth.512.-1 (upos)       1438             512   \n",
       "1            ewt.pyth.512.-1 (dep)       1438             512   \n",
       "2          ewt.pyth.512.-1 (other)       1438             512   \n",
       "3  programming_lang_id.pyth.512.-1       5397             512   \n",
       "4      natural_lang_id.pyth.512.-1      28084             512   \n",
       "5     text_features.pyth.256.10000      10000             256   \n",
       "6           counterfact.pyth.64.-1      43820              64   \n",
       "7      distribution_id.pyth.512.-1       8413             512   \n",
       "8        compound_words.pyth.24.-1     167959              24   \n",
       "9               latex.pyth.1024.-1       4486            1024   \n",
       "\n",
       "   Non padding tokens         Source  Average Class Balance  Total Features  \n",
       "0              281044            EWT                   0.20              16  \n",
       "1              281044            EWT                   0.20              29  \n",
       "2              281044            EWT                   0.20              22  \n",
       "3             2757867    pile-github                   0.11               9  \n",
       "4            14350924  pile-europarl                   0.11               9  \n",
       "5             2248714  pile-test-all                   0.26              11  \n",
       "6              403331  pile-test-all                   0.00              34  \n",
       "7             4299043  pile-test-all                   0.11               9  \n",
       "8             4031016  pile-test-all                   0.20              21  \n",
       "9             4589178     pile-arxiv                   0.26              12  "
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.set_option('max_colwidth', None)\n",
    "dataset_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\begin{tabular}{lrrrlrr}\n",
      "\\toprule\n",
      "                        Dataset &  Sequences &  Context Length &  Non padding tokens &        Source &  Average Class Balance &  Total Features \\\\\n",
      "\\midrule\n",
      "         ewt.pyth.512.-1 (upos) &       1438 &             512 &              281044 &           EWT &                   0.20 &              16 \\\\\n",
      "          ewt.pyth.512.-1 (dep) &       1438 &             512 &              281044 &           EWT &                   0.20 &              29 \\\\\n",
      "        ewt.pyth.512.-1 (other) &       1438 &             512 &              281044 &           EWT &                   0.20 &              22 \\\\\n",
      "programming\\_lang\\_id.pyth.512.-1 &       5397 &             512 &             2757867 &   pile-github &                   0.11 &               9 \\\\\n",
      "    natural\\_lang\\_id.pyth.512.-1 &      28084 &             512 &            14350924 & pile-europarl &                   0.11 &               9 \\\\\n",
      "   text\\_features.pyth.256.10000 &      10000 &             256 &             2248714 & pile-test-all &                   0.26 &              11 \\\\\n",
      "         counterfact.pyth.64.-1 &      43820 &              64 &              403331 & pile-test-all &                   0.00 &              34 \\\\\n",
      "    distribution\\_id.pyth.512.-1 &       8413 &             512 &             4299043 & pile-test-all &                   0.11 &               9 \\\\\n",
      "      compound\\_words.pyth.24.-1 &     167959 &              24 &             4031016 & pile-test-all &                   0.20 &              21 \\\\\n",
      "             latex.pyth.1024.-1 &       4486 &            1024 &             4589178 &    pile-arxiv &                   0.26 &              12 \\\\\n",
      "\\bottomrule\n",
      "\\end{tabular}\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/zl/vrkthn715j97ns3mht1gpfm80000gn/T/ipykernel_3380/1436211539.py:1: FutureWarning: In future versions `DataFrame.to_latex` is expected to utilise the base implementation of `Styler.to_latex` for formatting and rendering. The arguments signature may therefore change. It is recommended instead to use `DataFrame.style.to_latex` which also contains additional functionality.\n",
      "  dataset_latex_table = dataset_df.to_latex(index=False)\n"
     ]
    }
   ],
   "source": [
    "dataset_latex_table = dataset_df.to_latex(index=False)\n",
    "print(dataset_latex_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lowkey code duplication, I j copied the other table but selected different columns and deleted the columns we didn't need for this table \n",
    "def get_dataset_features(feature_datasets):\n",
    "    features_dict = {}\n",
    "    \n",
    "    for dataset in feature_datasets:\n",
    "        _fds = load_feature_dataset(dataset)\n",
    "        features_dict[dataset] = get_features(dataset)\n",
    "\n",
    "    dataset_data = []\n",
    "\n",
    "    ewt = load_feature_dataset('ewt.pyth.512.-1')\n",
    "    upos = filter_by_prefix(ewt.features, 'upos')\n",
    "    dep = filter_by_prefix(ewt.features, 'dep')\n",
    "    other = list(filter(lambda x: not x.startswith(\"upos\") and not x.startswith(\"dep\") and x.endswith('probe_indices'), ewt.features))\n",
    "    i = 5\n",
    "    for features, feature_name in [(set(upos), 'upos'), (set(dep), 'dep'), (set(other), 'other')]:\n",
    "        i = i + 1\n",
    "        features_list = set()\n",
    "        for feature in features:\n",
    "            features_list.add(feature.split('|')[0])\n",
    "        dataset_data.append({\n",
    "            'Dataset': f'ewt.pyth.512.-1 ({feature_name})',\n",
    "            'Features': unabbreviate_languages([item.split('|')[0] for item in features_list if item not in ['all_tokens', 'tokens', 'meta', 'text']])\n",
    "        })\n",
    "        \n",
    "    j = -1\n",
    "    for i, dataset in enumerate(feature_datasets):\n",
    "        if i == 5: continue \n",
    "        j = j + 1\n",
    "        fds = load_feature_dataset(dataset)\n",
    "        dataset_data.append({\n",
    "            'Dataset': dataset,\n",
    "            'Features': get_features(dataset)\n",
    "        })\n",
    "\n",
    "    dataset_df = pd.DataFrame(dataset_data, columns=['Dataset', 'Features'])\n",
    "    return dataset_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_names = get_dataset_features(feature_datasets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_names['Dataset'] = ['part of speech', 'dependencies', 'morphology', 'code language', 'natural language', 'text features', 'counterfact', 'datasubset', 'compound words', 'latex' ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_names = feature_names.drop(6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Dataset</th>\n",
       "      <th>Features</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>part of speech</td>\n",
       "      <td>upos_AUX, upos_ADP, upos_VERB, upos_ADJ, upos_X, upos_CCONJ, upos_PROPN, upos_NOUN, upos_INTJ, upos_SYM, upos_PRON, upos_DET, upos_SCONJ, upos_ADV, upos_PUNC, upos_NUM</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>dependencies</td>\n",
       "      <td>dep_aux:pass, dep_acl:relcl, dep_nsubj, dep_xcomp, dep_flat, dep_cc, dep_mark, dep_acl, dep_ccomp, dep_appos, dep_root, dep_nmod:poss, dep_aux, dep_amod, dep_nsubj:pass, dep_obj, dep_obl, dep_det, dep_advmod, dep_punct, dep_parataxis, dep_conj, dep_case, dep_list, dep_advcl, dep_cop, dep_compound, dep_nummod, dep_nmod</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>morphology</td>\n",
       "      <td>eos_True, Person_2, Gender_Fem, VerbForm_Inf, PronType_Dem, Gender_Masc, first_eos_True, Gender_Neut, VerbForm_Part, NumType_Card, PronType_Int, PronType_Prs, Person_3, Tense_Past, Number_Plur, PronType_Art, Voice_Pass, PronType_Rel, VerbForm_Ger, Mood_Imp, Person_1, VerbForm_Fin</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>code language</td>\n",
       "      <td>Python, XML, Java, C++, HTML, C, Go, PHP, JavaScript</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>natural language</td>\n",
       "      <td>Swedish, Portuguese, German, English, French, Spanish, Greek, Italian, Dutch</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>text features</td>\n",
       "      <td>leading_capital, no_leading_space_and_loweralpha, all_digits, is_not_ascii, has_leading_space, contains_all_whitespace, all_capitals, is_not_alphanumeric, contains_whitespace, contains_capital, contains_digit</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>datasubset</td>\n",
       "      <td>github, pubmed_abstracts, stack_exchange, wikipedia, freelaw, hackernews, arxiv, enron, uspto</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>compound words</td>\n",
       "      <td>mental-health, magnetic-field, trial-court, control-group, human-rights, north-america, clinical-trials, high-school, third-party, public-health, cell-lines, living-room, second-derivative, credit-card, social-media, prime-factors, federal-government, social-security, blood-pressure, gene-expression, side-effects</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>latex</td>\n",
       "      <td>is_superscript, is_inline_math, is_title, is_subscript, is_reference, is_denominator, is_author, is_numerator, is_display_math, is_math, is_abstract, is_frac</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            Dataset  \\\n",
       "0    part of speech   \n",
       "1      dependencies   \n",
       "2        morphology   \n",
       "3     code language   \n",
       "4  natural language   \n",
       "5     text features   \n",
       "7        datasubset   \n",
       "8    compound words   \n",
       "9             latex   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                          Features  \n",
       "0                                                                                                                                                          upos_AUX, upos_ADP, upos_VERB, upos_ADJ, upos_X, upos_CCONJ, upos_PROPN, upos_NOUN, upos_INTJ, upos_SYM, upos_PRON, upos_DET, upos_SCONJ, upos_ADV, upos_PUNC, upos_NUM  \n",
       "1  dep_aux:pass, dep_acl:relcl, dep_nsubj, dep_xcomp, dep_flat, dep_cc, dep_mark, dep_acl, dep_ccomp, dep_appos, dep_root, dep_nmod:poss, dep_aux, dep_amod, dep_nsubj:pass, dep_obj, dep_obl, dep_det, dep_advmod, dep_punct, dep_parataxis, dep_conj, dep_case, dep_list, dep_advcl, dep_cop, dep_compound, dep_nummod, dep_nmod  \n",
       "2                                         eos_True, Person_2, Gender_Fem, VerbForm_Inf, PronType_Dem, Gender_Masc, first_eos_True, Gender_Neut, VerbForm_Part, NumType_Card, PronType_Int, PronType_Prs, Person_3, Tense_Past, Number_Plur, PronType_Art, Voice_Pass, PronType_Rel, VerbForm_Ger, Mood_Imp, Person_1, VerbForm_Fin  \n",
       "3                                                                                                                                                                                                                                                                             Python, XML, Java, C++, HTML, C, Go, PHP, JavaScript  \n",
       "4                                                                                                                                                                                                                                                     Swedish, Portuguese, German, English, French, Spanish, Greek, Italian, Dutch  \n",
       "5                                                                                                                 leading_capital, no_leading_space_and_loweralpha, all_digits, is_not_ascii, has_leading_space, contains_all_whitespace, all_capitals, is_not_alphanumeric, contains_whitespace, contains_capital, contains_digit  \n",
       "7                                                                                                                                                                                                                                    github, pubmed_abstracts, stack_exchange, wikipedia, freelaw, hackernews, arxiv, enron, uspto  \n",
       "8       mental-health, magnetic-field, trial-court, control-group, human-rights, north-america, clinical-trials, high-school, third-party, public-health, cell-lines, living-room, second-derivative, credit-card, social-media, prime-factors, federal-government, social-security, blood-pressure, gene-expression, side-effects  \n",
       "9                                                                                                                                                                    is_superscript, is_inline_math, is_title, is_subscript, is_reference, is_denominator, is_author, is_numerator, is_display_math, is_math, is_abstract, is_frac  "
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.set_option('max_colwidth', None)\n",
    "feature_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\begin{tabular}{ll}\n",
      "\\toprule\n",
      "         Dataset &                                                                                                                                                                                                                                                                                                                        Features \\\\\n",
      "\\midrule\n",
      "  part of speech &                                                                                                                                                         upos\\_AUX, upos\\_ADP, upos\\_VERB, upos\\_ADJ, upos\\_X, upos\\_CCONJ, upos\\_PROPN, upos\\_NOUN, upos\\_INTJ, upos\\_SYM, upos\\_PRON, upos\\_DET, upos\\_SCONJ, upos\\_ADV, upos\\_PUNC, upos\\_NUM \\\\\n",
      "    dependencies & dep\\_aux:pass, dep\\_acl:relcl, dep\\_nsubj, dep\\_xcomp, dep\\_flat, dep\\_cc, dep\\_mark, dep\\_acl, dep\\_ccomp, dep\\_appos, dep\\_root, dep\\_nmod:poss, dep\\_aux, dep\\_amod, dep\\_nsubj:pass, dep\\_obj, dep\\_obl, dep\\_det, dep\\_advmod, dep\\_punct, dep\\_parataxis, dep\\_conj, dep\\_case, dep\\_list, dep\\_advcl, dep\\_cop, dep\\_compound, dep\\_nummod, dep\\_nmod \\\\\n",
      "      morphology &                                        eos\\_True, Person\\_2, Gender\\_Fem, VerbForm\\_Inf, PronType\\_Dem, Gender\\_Masc, first\\_eos\\_True, Gender\\_Neut, VerbForm\\_Part, NumType\\_Card, PronType\\_Int, PronType\\_Prs, Person\\_3, Tense\\_Past, Number\\_Plur, PronType\\_Art, Voice\\_Pass, PronType\\_Rel, VerbForm\\_Ger, Mood\\_Imp, Person\\_1, VerbForm\\_Fin \\\\\n",
      "   code language &                                                                                                                                                                                                                                                                            Python, XML, Java, C++, HTML, C, Go, PHP, JavaScript \\\\\n",
      "natural language &                                                                                                                                                                                                                                                    Swedish, Portuguese, German, English, French, Spanish, Greek, Italian, Dutch \\\\\n",
      "   text features &                                                                                                                leading\\_capital, no\\_leading\\_space\\_and\\_loweralpha, all\\_digits, is\\_not\\_ascii, has\\_leading\\_space, contains\\_all\\_whitespace, all\\_capitals, is\\_not\\_alphanumeric, contains\\_whitespace, contains\\_capital, contains\\_digit \\\\\n",
      "      datasubset &                                                                                                                                                                                                                                   github, pubmed\\_abstracts, stack\\_exchange, wikipedia, freelaw, hackernews, arxiv, enron, uspto \\\\\n",
      "  compound words &      mental-health, magnetic-field, trial-court, control-group, human-rights, north-america, clinical-trials, high-school, third-party, public-health, cell-lines, living-room, second-derivative, credit-card, social-media, prime-factors, federal-government, social-security, blood-pressure, gene-expression, side-effects \\\\\n",
      "           latex &                                                                                                                                                                   is\\_superscript, is\\_inline\\_math, is\\_title, is\\_subscript, is\\_reference, is\\_denominator, is\\_author, is\\_numerator, is\\_display\\_math, is\\_math, is\\_abstract, is\\_frac \\\\\n",
      "\\bottomrule\n",
      "\\end{tabular}\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/zl/vrkthn715j97ns3mht1gpfm80000gn/T/ipykernel_3380/3157256635.py:1: FutureWarning: In future versions `DataFrame.to_latex` is expected to utilise the base implementation of `Styler.to_latex` for formatting and rendering. The arguments signature may therefore change. It is recommended instead to use `DataFrame.style.to_latex` which also contains additional functionality.\n",
      "  feature_latex_table = feature_names.to_latex(index=False)\n"
     ]
    }
   ],
   "source": [
    "feature_latex_table = feature_names.to_latex(index=False)\n",
    "print(feature_latex_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/katherineharvey/sparse-probing-4/sparprob/lib/python3.9/site-packages/datasets/arrow_dataset.py:1533: FutureWarning: 'fs' was is deprecated in favor of 'storage_options' in version 2.8.0 and will be removed in 3.0.0.\n",
      "You can remove this warning by passing 'storage_options=fs.storage_options' instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['Number', 'Mood', 'Tense', 'VerbForm', 'PronType', 'Person', 'NumType', 'Voice', 'Gender', 'eos', 'first_eos', 'upos', 'dep', 'head', 'within_compound_token_ix', 'max_compound_token_ix', 'tokens', 'doc_id', 'split', 'position', 'upos_NOUN|probe_indices', 'upos_NOUN|probe_classes', 'upos_PUNC|probe_indices', 'upos_PUNC|probe_classes', 'upos_ADP|probe_indices', 'upos_ADP|probe_classes', 'upos_NUM|probe_indices', 'upos_NUM|probe_classes', 'upos_SYM|probe_indices', 'upos_SYM|probe_classes', 'upos_SCONJ|probe_indices', 'upos_SCONJ|probe_classes', 'upos_ADJ|probe_indices', 'upos_ADJ|probe_classes', 'upos_DET|probe_indices', 'upos_DET|probe_classes', 'upos_CCONJ|probe_indices', 'upos_CCONJ|probe_classes', 'upos_PROPN|probe_indices', 'upos_PROPN|probe_classes', 'upos_PRON|probe_indices', 'upos_PRON|probe_classes', 'upos_X|probe_indices', 'upos_X|probe_classes', 'upos_ADV|probe_indices', 'upos_ADV|probe_classes', 'upos_INTJ|probe_indices', 'upos_INTJ|probe_classes', 'upos_VERB|probe_indices', 'upos_VERB|probe_classes', 'upos_AUX|probe_indices', 'upos_AUX|probe_classes', 'dep_acl|probe_indices', 'dep_acl|probe_classes', 'dep_acl:relcl|probe_indices', 'dep_acl:relcl|probe_classes', 'dep_advcl|probe_indices', 'dep_advcl|probe_classes', 'dep_advmod|probe_indices', 'dep_advmod|probe_classes', 'dep_amod|probe_indices', 'dep_amod|probe_classes', 'dep_appos|probe_indices', 'dep_appos|probe_classes', 'dep_aux|probe_indices', 'dep_aux|probe_classes', 'dep_aux:pass|probe_indices', 'dep_aux:pass|probe_classes', 'dep_case|probe_indices', 'dep_case|probe_classes', 'dep_cc|probe_indices', 'dep_cc|probe_classes', 'dep_ccomp|probe_indices', 'dep_ccomp|probe_classes', 'dep_compound|probe_indices', 'dep_compound|probe_classes', 'dep_conj|probe_indices', 'dep_conj|probe_classes', 'dep_cop|probe_indices', 'dep_cop|probe_classes', 'dep_det|probe_indices', 'dep_det|probe_classes', 'dep_flat|probe_indices', 'dep_flat|probe_classes', 'dep_list|probe_indices', 'dep_list|probe_classes', 'dep_mark|probe_indices', 'dep_mark|probe_classes', 'dep_nmod|probe_indices', 'dep_nmod|probe_classes', 'dep_nmod:poss|probe_indices', 'dep_nmod:poss|probe_classes', 'dep_nsubj|probe_indices', 'dep_nsubj|probe_classes', 'dep_nsubj:pass|probe_indices', 'dep_nsubj:pass|probe_classes', 'dep_nummod|probe_indices', 'dep_nummod|probe_classes', 'dep_obj|probe_indices', 'dep_obj|probe_classes', 'dep_obl|probe_indices', 'dep_obl|probe_classes', 'dep_parataxis|probe_indices', 'dep_parataxis|probe_classes', 'dep_punct|probe_indices', 'dep_punct|probe_classes', 'dep_root|probe_indices', 'dep_root|probe_classes', 'dep_xcomp|probe_indices', 'dep_xcomp|probe_classes', 'VerbForm_Fin|probe_indices', 'VerbForm_Fin|probe_classes', 'VerbForm_Inf|probe_indices', 'VerbForm_Inf|probe_classes', 'VerbForm_Ger|probe_indices', 'VerbForm_Ger|probe_classes', 'VerbForm_Part|probe_indices', 'VerbForm_Part|probe_classes', 'PronType_Art|probe_indices', 'PronType_Art|probe_classes', 'PronType_Dem|probe_indices', 'PronType_Dem|probe_classes', 'PronType_Prs|probe_indices', 'PronType_Prs|probe_classes', 'PronType_Rel|probe_indices', 'PronType_Rel|probe_classes', 'PronType_Int|probe_indices', 'PronType_Int|probe_classes', 'Person_1|probe_indices', 'Person_1|probe_classes', 'Person_2|probe_indices', 'Person_2|probe_classes', 'Person_3|probe_indices', 'Person_3|probe_classes', 'Gender_Masc|probe_indices', 'Gender_Masc|probe_classes', 'Gender_Fem|probe_indices', 'Gender_Fem|probe_classes', 'Gender_Neut|probe_indices', 'Gender_Neut|probe_classes', 'Number_Plur|probe_indices', 'Number_Plur|probe_classes', 'Mood_Imp|probe_indices', 'Mood_Imp|probe_classes', 'Tense_Past|probe_indices', 'Tense_Past|probe_classes', 'NumType_Card|probe_indices', 'NumType_Card|probe_classes', 'Voice_Pass|probe_indices', 'Voice_Pass|probe_classes', 'eos_True|probe_indices', 'eos_True|probe_classes', 'first_eos_True|probe_indices', 'first_eos_True|probe_classes'],\n",
       "    num_rows: 1438\n",
       "})"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ewt = load_feature_dataset('ewt.pyth.512.-1')\n",
    "ewt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "fds = ewt\n",
    "features = []\n",
    "for feature in fds.features:\n",
    "    if feature.startswith('dep') and feature.endswith('probe_indices'):\n",
    "        features.append(feature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "59"
      ]
     },
     "execution_count": 170,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "59"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(filter_by_prefix(ewt.features, 'dep'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['VerbForm_Fin|probe_indices',\n",
       " 'VerbForm_Inf|probe_indices',\n",
       " 'VerbForm_Ger|probe_indices',\n",
       " 'VerbForm_Part|probe_indices',\n",
       " 'PronType_Art|probe_indices',\n",
       " 'PronType_Dem|probe_indices',\n",
       " 'PronType_Prs|probe_indices',\n",
       " 'PronType_Rel|probe_indices',\n",
       " 'PronType_Int|probe_indices',\n",
       " 'Person_1|probe_indices',\n",
       " 'Person_2|probe_indices',\n",
       " 'Person_3|probe_indices',\n",
       " 'Gender_Masc|probe_indices',\n",
       " 'Gender_Fem|probe_indices',\n",
       " 'Gender_Neut|probe_indices',\n",
       " 'Number_Plur|probe_indices',\n",
       " 'Mood_Imp|probe_indices',\n",
       " 'Tense_Past|probe_indices',\n",
       " 'NumType_Card|probe_indices',\n",
       " 'Voice_Pass|probe_indices',\n",
       " 'eos_True|probe_indices',\n",
       " 'first_eos_True|probe_indices']"
      ]
     },
     "execution_count": 176,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(filter(lambda x: not x.startswith(\"upos\") and not x.startswith(\"dep\") and x.endswith('probe_indices'), ewt.features))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 606,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Greek, German, French, Dutch, Spanish, Swedish, English, Portuguese, Italian'"
      ]
     },
     "execution_count": 606,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_df['Features'][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.unique(np.array(feature_dataset['lang']), return_counts=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 679,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.8080, 1.0000, 1.0000,  ..., 0.9860, 1.0000, 0.9743])"
      ]
     },
     "execution_count": 679,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "load_feature_dataset('programming_lang_id.pyth.512.-1')['lang_prob']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/katherineharvey/sparse-probing-4/sparprob/lib/python3.9/site-packages/datasets/arrow_dataset.py:1533: FutureWarning: 'fs' was is deprecated in favor of 'storage_options' in version 2.8.0 and will be removed in 3.0.0.\n",
      "You can remove this warning by passing 'storage_options=fs.storage_options' instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "load_feature_dataset('counterfact.pyth.64.-1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_dataset = load_feature_dataset('counterfact.pyth.64.-1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1090, 1838, 1640,  700,  866, 1026, 1428, 1694,  558,  854, 1848,\n",
       "       1512, 1750, 1822, 1156, 1558, 1238, 1632,  106, 1916, 1226, 1918,\n",
       "        278, 1502, 1782,  952,  432, 1904, 1588,  326, 1808,  632, 1548,\n",
       "       1692])"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.unique(np.array(feature_dataset['relation_id']), return_counts=True)[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 665,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9853899478912354"
      ]
     },
     "execution_count": 665,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(fds['lang_prob']).mean().item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 629,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['is_frac|probe_classes',\n",
       " 'is_numerator|probe_classes',\n",
       " 'is_denominator|probe_classes',\n",
       " 'is_title|probe_classes',\n",
       " 'is_abstract|probe_classes',\n",
       " 'is_author|probe_classes',\n",
       " 'is_subscript|probe_classes',\n",
       " 'is_superscript|probe_classes',\n",
       " 'is_reference|probe_classes',\n",
       " 'is_math|probe_classes',\n",
       " 'is_inline_math|probe_classes',\n",
       " 'is_display_math|probe_classes']"
      ]
     },
     "execution_count": 629,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_names = [name for name in fds.features if '|probe_classes' in name]\n",
    "feature_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "34"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(set(load_feature_dataset(feature_datasets[4])['relation_id']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['text', 'meta', 'lang_prob', 'lang', 'all_tokens', 'tokens', 'class_ids', 'probe_indices', 'valid_indices'],\n",
      "    num_rows: 5397\n",
      "})\n",
      "Dataset({\n",
      "    features: ['lang', 'tokens', 'class_ids', 'probe_indices', 'valid_indices'],\n",
      "    num_rows: 28084\n",
      "})\n",
      "Dataset({\n",
      "    features: ['tokens', 'distribution', 'abs_pos', 'norm_abs_pos', 'rel_pos', 'norm_rel_pos', 'log_pos', 'index_mask'],\n",
      "    num_rows: 10000\n",
      "})\n",
      "Dataset({\n",
      "    features: ['text', 'meta', 'all_tokens', 'tokens', 'contains_digit|probe_indices', 'contains_digit|probe_classes', 'all_digits|probe_indices', 'all_digits|probe_classes', 'contains_capital|probe_indices', 'contains_capital|probe_classes', 'leading_capital|probe_indices', 'leading_capital|probe_classes', 'all_capitals|probe_indices', 'all_capitals|probe_classes', 'contains_whitespace|probe_indices', 'contains_whitespace|probe_classes', 'has_leading_space|probe_indices', 'has_leading_space|probe_classes', 'no_leading_space_and_loweralpha|probe_indices', 'no_leading_space_and_loweralpha|probe_classes', 'contains_all_whitespace|probe_indices', 'contains_all_whitespace|probe_classes', 'is_not_alphanumeric|probe_indices', 'is_not_alphanumeric|probe_classes', 'is_not_ascii|probe_indices', 'is_not_ascii|probe_classes'],\n",
      "    num_rows: 10000\n",
      "})\n",
      "Dataset({\n",
      "    features: ['relation_id', 'text', 'text_true', 'tokens', 'probe_indices', 'valid_indices'],\n",
      "    num_rows: 43820\n",
      "})\n",
      "Dataset({\n",
      "    features: ['text', 'meta', 'all_tokens', 'tokens', 'distribution', 'probe_indices', 'valid_indices'],\n",
      "    num_rows: 8413\n",
      "})\n",
      "Dataset({\n",
      "    features: ['Number', 'Mood', 'Tense', 'VerbForm', 'PronType', 'Person', 'NumType', 'Voice', 'Gender', 'eos', 'first_eos', 'upos', 'dep', 'head', 'within_compound_token_ix', 'max_compound_token_ix', 'tokens', 'doc_id', 'split', 'position', 'upos_NOUN|probe_indices', 'upos_NOUN|probe_classes', 'upos_PUNC|probe_indices', 'upos_PUNC|probe_classes', 'upos_ADP|probe_indices', 'upos_ADP|probe_classes', 'upos_NUM|probe_indices', 'upos_NUM|probe_classes', 'upos_SYM|probe_indices', 'upos_SYM|probe_classes', 'upos_SCONJ|probe_indices', 'upos_SCONJ|probe_classes', 'upos_ADJ|probe_indices', 'upos_ADJ|probe_classes', 'upos_DET|probe_indices', 'upos_DET|probe_classes', 'upos_CCONJ|probe_indices', 'upos_CCONJ|probe_classes', 'upos_PROPN|probe_indices', 'upos_PROPN|probe_classes', 'upos_PRON|probe_indices', 'upos_PRON|probe_classes', 'upos_X|probe_indices', 'upos_X|probe_classes', 'upos_ADV|probe_indices', 'upos_ADV|probe_classes', 'upos_INTJ|probe_indices', 'upos_INTJ|probe_classes', 'upos_VERB|probe_indices', 'upos_VERB|probe_classes', 'upos_AUX|probe_indices', 'upos_AUX|probe_classes', 'dep_acl|probe_indices', 'dep_acl|probe_classes', 'dep_acl:relcl|probe_indices', 'dep_acl:relcl|probe_classes', 'dep_advcl|probe_indices', 'dep_advcl|probe_classes', 'dep_advmod|probe_indices', 'dep_advmod|probe_classes', 'dep_amod|probe_indices', 'dep_amod|probe_classes', 'dep_appos|probe_indices', 'dep_appos|probe_classes', 'dep_aux|probe_indices', 'dep_aux|probe_classes', 'dep_aux:pass|probe_indices', 'dep_aux:pass|probe_classes', 'dep_case|probe_indices', 'dep_case|probe_classes', 'dep_cc|probe_indices', 'dep_cc|probe_classes', 'dep_ccomp|probe_indices', 'dep_ccomp|probe_classes', 'dep_compound|probe_indices', 'dep_compound|probe_classes', 'dep_conj|probe_indices', 'dep_conj|probe_classes', 'dep_cop|probe_indices', 'dep_cop|probe_classes', 'dep_det|probe_indices', 'dep_det|probe_classes', 'dep_flat|probe_indices', 'dep_flat|probe_classes', 'dep_list|probe_indices', 'dep_list|probe_classes', 'dep_mark|probe_indices', 'dep_mark|probe_classes', 'dep_nmod|probe_indices', 'dep_nmod|probe_classes', 'dep_nmod:poss|probe_indices', 'dep_nmod:poss|probe_classes', 'dep_nsubj|probe_indices', 'dep_nsubj|probe_classes', 'dep_nsubj:pass|probe_indices', 'dep_nsubj:pass|probe_classes', 'dep_nummod|probe_indices', 'dep_nummod|probe_classes', 'dep_obj|probe_indices', 'dep_obj|probe_classes', 'dep_obl|probe_indices', 'dep_obl|probe_classes', 'dep_parataxis|probe_indices', 'dep_parataxis|probe_classes', 'dep_punct|probe_indices', 'dep_punct|probe_classes', 'dep_root|probe_indices', 'dep_root|probe_classes', 'dep_xcomp|probe_indices', 'dep_xcomp|probe_classes', 'VerbForm_Fin|probe_indices', 'VerbForm_Fin|probe_classes', 'VerbForm_Inf|probe_indices', 'VerbForm_Inf|probe_classes', 'VerbForm_Ger|probe_indices', 'VerbForm_Ger|probe_classes', 'VerbForm_Part|probe_indices', 'VerbForm_Part|probe_classes', 'PronType_Art|probe_indices', 'PronType_Art|probe_classes', 'PronType_Dem|probe_indices', 'PronType_Dem|probe_classes', 'PronType_Prs|probe_indices', 'PronType_Prs|probe_classes', 'PronType_Rel|probe_indices', 'PronType_Rel|probe_classes', 'PronType_Int|probe_indices', 'PronType_Int|probe_classes', 'Person_1|probe_indices', 'Person_1|probe_classes', 'Person_2|probe_indices', 'Person_2|probe_classes', 'Person_3|probe_indices', 'Person_3|probe_classes', 'Gender_Masc|probe_indices', 'Gender_Masc|probe_classes', 'Gender_Fem|probe_indices', 'Gender_Fem|probe_classes', 'Gender_Neut|probe_indices', 'Gender_Neut|probe_classes', 'Number_Plur|probe_indices', 'Number_Plur|probe_classes', 'Mood_Imp|probe_indices', 'Mood_Imp|probe_classes', 'Tense_Past|probe_indices', 'Tense_Past|probe_classes', 'NumType_Card|probe_indices', 'NumType_Card|probe_classes', 'Voice_Pass|probe_indices', 'Voice_Pass|probe_classes', 'eos_True|probe_indices', 'eos_True|probe_classes', 'first_eos_True|probe_indices', 'first_eos_True|probe_classes'],\n",
      "    num_rows: 1438\n",
      "})\n",
      "Dataset({\n",
      "    features: ['tokens', 'label', 'feature_name'],\n",
      "    num_rows: 167959\n",
      "})\n",
      "Dataset({\n",
      "    features: ['tokens', 'is_frac|probe_indices', 'is_frac|probe_classes', 'is_frac|valid_indices', 'is_numerator|probe_indices', 'is_numerator|probe_classes', 'is_numerator|valid_indices', 'is_denominator|probe_indices', 'is_denominator|probe_classes', 'is_denominator|valid_indices', 'is_title|probe_indices', 'is_title|probe_classes', 'is_title|valid_indices', 'is_abstract|probe_indices', 'is_abstract|probe_classes', 'is_abstract|valid_indices', 'is_author|probe_indices', 'is_author|probe_classes', 'is_author|valid_indices', 'is_subscript|probe_indices', 'is_subscript|probe_classes', 'is_subscript|valid_indices', 'is_superscript|probe_indices', 'is_superscript|probe_classes', 'is_superscript|valid_indices', 'is_reference|probe_indices', 'is_reference|probe_classes', 'is_reference|valid_indices', 'is_math|probe_indices', 'is_math|probe_classes', 'is_math|valid_indices', 'is_inline_math|probe_indices', 'is_inline_math|probe_classes', 'is_inline_math|valid_indices', 'is_display_math|probe_indices', 'is_display_math|probe_classes', 'is_display_math|valid_indices'],\n",
      "    num_rows: 4486\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "for dataset in feature_datasets:\n",
    "    loaded_dataset = load_feature_dataset(dataset)\n",
    "    print(loaded_dataset)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "for dataset in feature_datasets:\n",
    "    fds = load_feature_dataset(dataset)\n",
    "    print(dataset, fds.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 346,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'arxiv',\n",
       " 'enron',\n",
       " 'freelaw',\n",
       " 'github',\n",
       " 'hackernews',\n",
       " 'pubmed_abstracts',\n",
       " 'stack_exchange',\n",
       " 'uspto',\n",
       " 'wikipedia'}"
      ]
     },
     "execution_count": 346,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "set(load_feature_dataset(feature_datasets[5])['distribution'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fds = load_feature_dataset(feature_datasets[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['lang', 'tokens', 'class_ids', 'probe_indices', 'valid_indices'],\n",
       "    num_rows: 28084\n",
       "})"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fds['text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'C', 'C++', 'Go', 'HTML', 'Java', 'JavaScript', 'PHP', 'Python', 'XML'}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "set(fds['lang'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5397, 512])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fds['tokens'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2757867"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(fds['tokens'] > 1).numpy().sum()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.7 ('sparprob': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "a23c353b8f126b39e08e10b291cdc446f934514e233cf645ae9c7e400e436e5b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
